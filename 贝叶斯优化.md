### 贝叶斯优化

贝叶斯优化是一种黑盒优化算法，用于求解表达式未知的函数的极值问题。算法根据一组采样点处的函数值预测出任意点处函数值的概率分布，这通过高斯过程回归而实现。根据高斯过程回归的结果构造采集函数，用于衡量每一个点值得探索的程度，求解采集函数的极值从而确定下一个采样点。最后返回这组采样点的极值作为函数的极值。这种算法在机器学习中被用于AutoML算法，自动确定机器学习算法的超参数。

#### 1 黑盒优化问题

绝大多数机器学习算法都有超参数。这些超参数可以分为两种类型，定义模型及结构本身的参数，目标函数与优化算法（求解器）所需的参数。前者用于训练和预测阶段，后者只用于训练阶段。在训练时需要人工设定它们的值，通过反复试验获得好的结果，整个过程会耗费大量的时间和人力成本。因此如何自动确定超参数的值是AutoML中一个重要的问题。问题的核心是自动搜索出最优超参数值以最大化预期目标。因此可抽象为函数极值问题，优化变量为超参数，函数值为机器学习模型的性能指标如准确率、预测速度。

黑盒优化问题目标函数的表达式未知，只能根据离散的自变量取值得到对应的目标函数值。**超参数优化属于黑盒优化问题，在优化过程中只能得到函数的输入和输出，不能获取优化目标函数的表达式和梯度信息**，这一特点给超参数优化带来了困难。对于某些机器学习模型，超参数数量较大，是高维优化问题。对模型进行评估即计算目标函数的值在很多情况下成本高昂，因为这意味着要以某种超参数配置训练机器学习模型，并在验证集上计算精度等指标。常用的超参数优化方法有网格搜索（Grid search），随机搜索（Random search），遗传算法，贝叶斯优化（Bayesian Optimization）等，接下来分别进行介绍。

##### 1.1网格搜索 grid search

网格搜索是最简单的做法，它搜索一组离散的取值情况，得到最优参数值。对于连续型的超参数，对其可行域进行网格划分，选取一些典型值进行计算。假设需要确定的超参数有2个，第1个的取值为[0,1]之间的实数，第2个的取值为[1,2]之间的实数。则可以按照如下的方案得到若干离散的取值，以这些值运行算法：

将第1个参数均匀的取3个典型值，将第2个参数均匀的取3个典型值。对于所有的取值组合运行算法，将性能最优的取值作为超参数的最终取值。这种方法如图1所示。

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/y98ndw4gz2.png)

##### 1.2随机搜索 random search

随机搜索做法是将超参数随机地取某些值，比较各种取值时算法的性能，得到最优超参数值，其原理如图2所示。

![img](C:\Users\cimum\Documents\CV\wfzmkort6t.png)

##### 1.3贝叶斯优化 Bayesian Optimization

网格搜索和随机搜索**没有利用已搜索点的信息**，使用这些信息指导搜索过程可以提高结果的质量以及搜索的速度。贝叶斯优化（Bayesian optimization algorithm，简称BOA）**利用之前已搜索点的信息确定下一个搜索点**，用于求解维数不高的黑盒优化问题。

###### 思路：

算法的思路是首先生成一个**初始候选解集合**【通常是均匀的】，然后根据这些点**寻找下一个有可能是极值的点**，将该点加入集合中，重复这一步骤，直至迭代终止【终止条件是迭代次数】。最后从这些点中找出极值点作为问题的解。

这里的关键问题是**如何根据已经搜索的点确定下一个搜索点**。**贝叶斯优化根据已经搜索的点的函数值估计真实目标函数值的均值和方差（即波动范围）**，如图3所示。上图中红色的曲线为估计出的目标函数值即在每一点出处的目标函数值的均值。现在有3个已经搜索的点，用黑色实心点表示。**两条虚线所夹区域为在每一点处函数值的变动范围，在以均值即红色曲线为中心，与标准差成正比的区间内波动。在搜索点处，红色曲线经过搜索点，且方差最小，在远离搜索点处方差更大**，这也符合我们的直观认识，远离采样点处的函数值估计的更不可靠。

根据均值和方差可以构造出采集函数（acquisition function），即对每一点是函数极值点的可能性的估计，反映了每一个点值得搜索的程度，该函数的**极值点是下一个搜索点**，如图3的下图所示。下图中的矩形框所表示的点是采集函数的极大值点，也是下一个搜索点。

算法的核心由两部分构成：

- 对目标函数进行建模即计算每一点处的函数值的均值和方差，通常用高斯过程回归实现；
- 构造采集函数，用于决定本次迭代时在哪个点处进行采样。

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/prz8splevz.png)

#### 2 高斯过程回归

##### 2.1 高斯过程

多维高斯分布具有诸多优良的性质。高斯过程（Gaussian Process，GP）用于对一组随着时间增长的随机向量进行建模，在任意时刻随机向量的所有子向量均服从高斯分布。假设有连续型随机变量序列$x_1,...,x_T$，，如果该序列中任意数量的随机变量构成的向量$x_{t_1,...,t_k} = [x_{t_1} \ ...\ x_{t_k}]^T$均服从多维正态分布，则称此随机变量序列为高斯过程。

特别地，假设当前有k个随机变量$x_1,...,x_k$，它们服从k维正态分布$N(\mu_k,\sum_k)$，其中均值向量$\mu_k \isin \mathbb{R}^k$，协方差矩阵$\sum_k \isin \mathbb{R}^{k\times k}$。

加入一个新的随机变量$x_{k+1}$之后，随机向量$x_1,...,x_k,x_{k+1}$服从k+1维正态分布$N(\mu_{k+1},\sum_{k+1})$。其中均值向量$\mu_{k+1} \isin \mathbb{R}^{k+1}$，协方差矩阵$\sum_{k+1} \isin \mathbb{R}^{(k+1)\times (k+1)}$。

由于正态分布的积分能得到解析解，因此可以方便地得到边缘概率与条件概率。均值向量与协方差矩阵的计算将在稍后讲述。

##### 2.2 高斯过程回归 Gaussian Process Regression，GPR

在机器学习中，算法通常情况下是根据输入值x预测出一个最佳输出值y，用于分类或回归任务。这种情况将y看作普通的变量。某些情况下我们需要的不是预测出一个函数值，而是给出这个函数值的后验概率分布$P(y|x)$。此时将函数值看作随机变量。对于实际应用问题，一般是给定一组样本点$x_i,\ i=1,...,l$，根据它们拟合出一个假设函数，给定输入值x，预测其标签值y或者其后验概率$P(y|x)$。

高斯过程回归对应的是第二种方法。

高斯过程回归（Gaussian Process Regression，GPR）**对表达式未知的函数（黑盒函数）的一组函数值进行贝叶斯建模，给出函数值的概率分布**。

假设有黑盒函数f(x)实现如下映射$\mathbb{R}^n \rightarrow \mathbb{R}$。高斯过程回归可以根据某些点$x_i,\ i=1,...,l$，以及在这些点处的函数值$f(x_i)$得到一个模型，拟合此黑盒函数。对于任意给定的输入值$x$可以预测出$f(x)$，并给出预测结果的置信度。**事实上模型给出的是$f(x)$的概率分布**。

高斯过程回归假设黑盒函数在各个点处的函数值$f(x)$都是随机变量，它们构成的随机向量服从多维正态分布。对于函数$f(x)$，$x$有若干个采样点$x_1,...,x_t$（后续用$x_{1:t}$表示），在这些点处的函数值构成向量$f(x_{1:t}) = [f(x_1) \ \ ...\ \ f(x_t)]$。

高斯过程回归假设此向量服从k维正态分布$f(x_{1:t})~N(\mu(x_{1:t}),\sum(x_{1:t},x_{1:t}))$。

其中：

- $\mu(x_{1:t})$是高斯分布的均值向量
  - $\mu(x_{1:t})=[\mu(x_1) \ \ ... \ \ \mu(x_t)]$
- $\sum(x_{1:t},x_{1:t})$是协方差矩阵
  - $\begin{equation} \left[ \begin{array}{ccc} \text{cov}(X_1, X_1) & \cdots & \text{cov}(X_1, X_t) \\ \vdots & \ddots & \vdots \\ \text{cov}(X_t, X_1) & \cdots & \text{cov}(X_t, X_t) \\ \end{array} \right] = \left[ \begin{array}{ccc} k(X_1, X_1) & \cdots & k(X_1, X_t) \\ \vdots & \ddots & \vdots \\ k(X_t, X_1) & \cdots & k(X_t, X_t) \\ \end{array} \right] \end{equation}$，其中k代表高斯核函数

这样问题的**核心**就是**如何根据样本值计算出正态分布的均值向量和协方差矩阵**。

- 均值向量通过使用均值函数$\mu (x)$根据每个采样点$x$计算而构造。

- 协方差通过核函数$k(x,x')$根据样本点对 $x,x'$ 计算得到，也称为协方差函数。核函数需要满足下面的要求。



核函数$k$需要满足以下要求：

- 距离相近的样本点$x$和$x'$之间有更大的正协方差值，因为相近的两个点的函数值也相似，有更强的相关性
- 保证协方差矩阵是对称半正定矩阵。根据任意一组样本点计算出的协方差矩阵都必须是对称**半正定矩阵**。





在这样的条件下，通常使用的是高斯核与Matern核。

高斯核定义为
$$
k(x_1,x_2)=\alpha_0 exp(-\frac{1}{2\sigma^2}||x_1-x_2||^2)
$$
其中$\alpha_0,\sigma$为核函数的参数。显然该核函数满足上面的要求。

Matern核定义为
$$
k(x_1,x_2)=\frac{2^{1-v}}{\Gamma(v)}(\sqrt{2v}||x_1-x_2||)^vK_v(\sqrt{2v}||x_1-x_2||))
$$
其中$\Gamma$是伽马函数，$K_v$是贝塞尔函数（Bessel function），$v$是人工设定的正参数。用核函数计算任意两点之间的核函数值，得到核函数矩阵K作为协方差矩阵的估计值。
$$

K =
\begin{bmatrix}
k(X_{1}, X_{1}) & \cdots & k(X_{1}, X_{t}) \\
\vdots & \ddots & \vdots \\
k(X_{t}, X_{1}) & \cdots & k(X_{t}, X_{t})
\end{bmatrix}
$$


利用**核函数得到协方差**后，可以使用常数函数**得到均值**$\mu(x)=c$或简单地将均值统一设置为0。即使将均值统一设置为常数，因为有方差的作用，依然能够对数据进行有效建模。如果知道目标函数f(x)的结构，也可以使用更复杂的函数。



在计算出均值向量与协方差矩阵之后，可以根据此多维正态分布来预测f(x)在任意点处函数值的概率分布。

假设已经得到了一组样本值$x_{1:t}$以及其对应的函数值$f(x_{1:t})$，接下来要预测新的点x的函数值$f(x)$的数学期望$\mu(x)$和方差$\sigma^2(x)$。

如果令$x_{t+1}=x$，则加入该点之后$f(x_{1:t+1})$服从t+1维正态正态分布。

将均值向和协方差矩阵进行分块，可以写成：
$$
\begin{bmatrix}
f(\textbf x_{t}) \\
f(\textbf x_{t+1})
\end{bmatrix}
\sim
\mathcal{N}
\left(
\begin{bmatrix}
\mu(\textbf x_{t}) \\
\mu(\textbf x_{t+1})
\end{bmatrix},
\begin{bmatrix}
\textbf K & \textbf k \\
\textbf k^T & k(\textbf x_{t+1},\textbf x_{t+1})
\end{bmatrix}
\right)
$$
其中：

- $f(\textbf x_{1:t})$：
  - 服从t维正态正态分布，其**均值**向量为$\mu(\textbf x_{1:t})$，**协方差矩阵**为**$\textbf K$**，它们可以利用样本集 $\textbf x_i,i=1,...,t$ 根据均值函数和协方差函数算出。
- $f(\textbf x_{t+1})$：
  - 其**均值**向量为$\mu(\textbf x_{t+1})$，**协方差向量**为**$\textbf k$**。$k$是$t$维列向量，根据$\textbf x_{t+1}$与$\textbf x_1,...,\textbf x_t$使用核函数计算：$\textbf k = [k(\textbf x_{t+1},x_1)\ \ ... \ \ k(\textbf x_{t+1},\textbf x_t)]$

- $k(\textbf x_{t+1},\textbf x_{t+1})$可以通过核函数算出



发现其实并没有使用到$f(\textbf x_{t+1})$的值，它们在计算新样本点的条件概率时才会被使用。



多维正态分布的条件分布仍为正态分布。可以计算出在已知$f(x_{1:t})$的情况下$f(x_{t+1})$所服从的条件分布，根据多维正态分布的性质，它服从一维正态分布$f(x_{t+1})|f(x_{1:t}) \sim N(\mu,\sigma^2)$

对于前面介绍的均值向量和协方差矩阵分块方案，根据多维正态分布条件分布的计算公式，可以计算出此条件分布的均值和方差。计算公式为
$$
\mu = \textbf k^T \textbf K^{-1} (f(\textbf x_{t:}) - \mu(\textbf x_{t:})) + \mu(\textbf x_{t+1})
\newline
\sigma^2 = \textbf k(\textbf x_{t+1}, \textbf x_{t+1}) - \textbf k^T\textbf  K^{-1}\textbf  k
$$

#### 3 贝叶斯优化

贝叶斯优化的思路是首先**生成一个初始候选解集合**，然后**根据这些点寻找下一个最有可能是极值的点，将该点加入集合中，重复这一步骤，直至迭代终止**。最后从这些点中找出函数值最大的点作为问题的解。由于求解过程中利用之前已搜索点的信息，因此比网格搜索和随机搜索更为有效。

这里的**关键**问题是**如何根据已经搜索的点确定下一个搜索点，通过高斯过程回归和采集函数实现**。高斯过程回归根据已经搜索的点估计其他点处目标函数值的均值和方差，如图5所示。图5中蓝色实线为真实的目标函数曲线，黑色虚线为算法估计出的在每一点处的目标函数值。图中有7个已经搜索的点，用红色点表示。蓝色带状区域为在每一点处函数值的置信区间。函数值在以均值，即黑色虚线为中心，与标准差成正比的区间内波动。图5的下图为采集函数曲线，下一个采样点为采集函数的极大值点，以五角星表示。

在已搜索点处，黑色虚线经过这些点，且方差最小；在远离搜索点处方差更大。这也符合我们的直观认识，远离采样点处的函数值估计的更不可靠。根据均值和方差构造出采集函数，是对每一点是函数极值可能性的估计，反映了该点值得搜索的程度。该函数的极值点即为下一个搜索点。贝叶斯优化算法的流程如下所示。

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/sqfhhoxgpw.png)

其核心由两部分构成：

1. 高斯过程回归。计算每一点处函数值的均值和方差；
2. 根据均值和方差构造采集函数，用于决定本次迭代时在哪个点处进行采样。

算法首先初始化

![img](https://ask.qcloudimg.com/http-save/yehe-2448033/x2ydoahl1m.png)

个候选解，通常在整个可行域内**均匀地**选取一些点。然后开始循环，**每次增加一个点**，直至找到N个候选解。每次寻找下一个点时，**用已经找到的n个候选解建立高斯回归模型，得到任意点处的函数值的后验概率**。然后根据后验概率**构造采集函数**，**寻找函数的极大值点作为下一个搜索点**。接下来计算在下一个搜索点处的函数值。**算法最后返回N个候选解的极大值作为最优解。**



![9h76hxkqr0](C:\Users\cimum\Downloads\9h76hxkqr0.png)

https://cloud.tencent.com/developer/article/1641595