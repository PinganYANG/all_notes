#### 如何解决RNN梯度消失或梯度爆炸

对于梯度爆炸而言，利用gradient clipping方法，当梯度过大或过小时，直接截断

对于梯度消失而言，可以将sigmoid或tanh激活函数改为relu

#### RNN的经典结构

1对1 输入和输出一致，比如文本对应

1对多 图形生成文字，生成音乐等

多对多 机器翻译等

#### LSTM为什么存在两种激活函数sigmoid和tanh

​	因为sigmoid用于各个门上，用于判断信息记忆还是遗忘

​	而tanh作用于状态和输出上

#### LSTM结构

![img](https://pic3.zhimg.com/80/v2-ff1e53c5716da3fc54ed2578fec905f6_720w.webp)

三个门

输入 用sigmoid值决定更新长期记忆Ct中的哪些部分，并用它来处理由短期记忆和输入X决定的长期记忆更改部分

遗忘 看看对上一个Ct要遗忘什么

输出 输出给下一个短期记忆

#### GRU结构

![img](https://pic3.zhimg.com/80/v2-5b805241ab36e126c4b06b903f148ffa_720w.webp)

**更新门** 用xt和ht-1来计算需要更新ht-1的部分

**重置门** 判断过去多少信息需要被遗忘

用充值后的和更新后的得到记忆和输出





#### 样本不均衡怎么解决

首先如果样本是简单的线性可分的，那么其实影响不大。

否则

1） 可以进行欠采样（随机欠采样，ENN **Edited Nearest Neighbors** 对于数据集中的每一个实例，使用k-最近邻算法来找到它的k个最近邻。如果一个实例的大部分最近邻属于其他类别，那么这个实例将被删除。）或过采样方法（随机过采样）。

2） 可以进行data augmentation 

3） 可以在损失函数方面进行修改

​       可以用class weight 方法，为不同类别提供不同权重

​        OHEM（Online Hard Example Mining）方法OHEM 的方法是在每次迭代中，首先使用当前的模型对整个数据集进行前向传播，然后根据损失值选择一些“困难”的样本进行反向传播。这样，模型的更新将主要集中在那些模型当前处理得不好的样本上。

​       Focal loss 特别设计用于处理高度不平衡的分类问题 

​       $FL(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)$ 这样pt是被分为正分类的概率。当概率pt接近1时，即分类正确时，就会使得损失值接近0，而概率接近0时，即分类错误时，则几乎不影响损失值，整体而言，相当于增加了分类不准确样本在损失函数中的权重。而α则直接影响正负样本权重。即通过αt 可以抑制正负样本的数量失衡，通过 γ 可以控制简单/难区分样本数量失衡。

4） 模型方面可以采用对样本不均衡不敏感的模型，比如LR或决策树，集成学习树模型。

5） 直接将其转化为一个异常检测问题

#### LSTM如何解决梯度消失的问题/为什么比RNN好

LSTM增加了更多回传梯度的路径，只要一条路径没有梯度消失，那么梯度消失的问题就得到了改善。因为LSTM有进有出且当前的**cell** informaton是通过input gate控制之后**叠加**的，**RNN是叠乘**，因此LSTM可以防止梯度消失或者爆炸。



#### 还有哪些其它的解决梯度消失或梯度爆炸的方法？

- 梯度裁剪gradient clipping，当BP时的梯度小于某个阈值或大于某个阈值时 ，直接裁剪，防止太小的梯度累乘带来的梯度消失或太大的梯度累乘带来的梯度爆炸。
- 改变激活函数，例如减少使用sigmoid、tanh这类激活函数，改成使用Relu、LeakRelu等，参考 [算法面试问题二（激活函数相关）](https://zhuanlan.zhihu.com/p/354013996) 。
- 残差结构，类似于CEC的模块，跨层的连接结构能让梯度无损的进行后向传播。
- Batch Normalization，相当于对每一层的输入做了一个规范化，强行把这个输入拉回标准正态分布*N~(0,1)。*这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数的大变化，进而梯度变大，避免产生梯度消失问题。而且梯度变化大 意味着学习收敛速度快，加快模型的训练速度。

#### Batch norm 和 layer norm 的区别

1. **计算轴的区别**：
   - **Batch Normalization (BN)**：BN 是沿着小批量数据的维度（通常是第0维）进行正规化的。换句话说，它独立地计算每个特征的均值和方差，基于整个小批量数据。【同时考虑每个样本中不同feature
   - **Layer Normalization (LN)**：LN 是沿着特征的维度进行正规化的。对于每个数据点，它计算所有特征的均值和方差。【在同一个样本之中共同考虑不同feature
2. **统计特性**：
   - **Batch Normalization (BN)**：因为 BN 是基于小批量数据进行计算的，所以它的统计特性（均值和方差）会随着每个小批量的数据变化而变化。
   - **Layer Normalization (LN)**：LN 的统计特性是固定的，因为它是基于单个数据点的所有特征进行计算的。
3. **使用场景**：
   - **Batch Normalization (BN)**：BN 主要用于前馈神经网络和卷积神经网络中【可以增加效率 加快收敛速度。
   - **Layer Normalization (LN)**：LN 通常用于循环神经网络（RNN）和 Transformer 结构中，因为它不依赖于小批量的大小，这使得它在处理序列数据时特别有用。【即使是单个输出也可以做norm
4. **外部参数**：
   - 无论是 BN 还是 LN，都有可学习的缩放和偏移参数，用于进一步调整正规化后的输出。
5. **对小批量大小的依赖**：
   - **Batch Normalization (BN)**：BN 对小批量的大小非常敏感。太小的小批量可能导致均值和方差的不稳定估计。
   - **Layer Normalization (LN)**：LN 完全不依赖于小批量的大小，这使得它在小批量大小变化时更为稳定。

#### CNN RNN 调参

momemtum 动量

epoch

learning rate

weight init （Xavier init）

regularization

​	dropout L1 L2

gradient clipping 

early stopping



num of layers

num of nodes

num of filters



#### 为什么用非线性激活函数

如果不适用非线性函数，多层的layer就毫无用处了

#### **什么是梯度爆炸**

误差梯度是神经网络训练过程中计算的方向和数量，用于以正确的方向和合适的量更新网络权重。

在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。

网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。

#### **梯度爆炸会引起什么问题**

在深度多层感知机网络中，梯度爆炸会引起网络不稳定，最好的结果是无法从训练数据中学习，而最坏的结果是出现无法再更新的 NaN 权重值。

#### One-stage

直接预测物体的类别和边框位置，比如YOLO【YOLO预测（c,x,y,w,h)】和SSD。

#### Two-stage

生成预选框，然后在预选框中预测类别和偏移量，比如RCNN家族

#### One-stage vs. Two-stage

- one-stage 速度快，精度低 vs two-stage 速度慢，精度高
- one-stage 实时响应 vs two-stage 准确性更高的

#### 交叉熵损失

$$
\begin{equation}
CE(p,y) = 
\begin{cases} 
-log(p) & \text{if } y = 1 \\
-log(1-p) & \text{otherwise }  
\end{cases}
\end{equation}
$$

$CE(p,y) = CE(p_t) = -log(p_t) = -ylog(p) - (1-y)log(1-p)$
$$
\begin{equation}
p_t = 
\begin{cases} 
p & \text{if } y = 1 \\
1-p & \text{otherwise }  
\end{cases}
\end{equation}
$$






#### Focal Loss

One-stage方法精度低的一个原因就是正负样本**极度不平衡**



【

**极度不平衡**：

1. 目标检测算法为了定位目标会生成大量的anchor box
2. 而一幅图中目标(正样本)个数很少，大量的anchor box处于背景区域(负样本)，这就导致了正负样本极不平衡

】



来源于**交叉熵损失**

那么怎么由交叉熵损失改进为Focal loss的？

首先我们的目标是处理正负样本**极度不平衡**，所以第一步：

- 为正样本增加权重α
  - $CE(p,y)  = -\alpha ylog(p) - (1-\alpha)(1-y)log(1-p)$
  - 这个方法虽然增加了正样本权重，但无法控制正样本中，**易分类样本**和**难分类样本**的权重
- 增加了针对难易样本的调制系数$(1-p_t)^\gamma$
  - 当一个样本被分错（**难样本**）的时候，此时$p_t$非常小，那么调制系数$(1-p_t)^\gamma$接近 1，损失基本不会被影响；
  - 当遇到一个**易样本**的时候， $p_t\rightarrow 1$，因此有 $(1-p_t)^\gamma\rightarrow 1$ ，那么对于比较容易的样本，loss 就会降低，相当于权重降低；

最终结果为：

$FL(p_t) = -\alpha_t(1-p_t)^\gamma log(p_t) $